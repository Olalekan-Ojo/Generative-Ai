{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Olalekan-Ojo/Generative-Ai/blob/main/ITM%20-%20Image%20Text%20Matching.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fYMEeyo9BOdP"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# Image-Text Matching Classifier: baseline system for visual question answering\n",
        "#\n",
        "# This program has been adapted and rewriten from the CMP9137 materials of 2024.\n",
        "#\n",
        "# It treats the task of multi-choice visual question answering as a binary\n",
        "# classification task. This is possible by rewriting the questions from this format:\n",
        "# v7w_2358727.jpg\tWhen was this?  Nighttime. | Daytime. | Dawn. Sunset.\n",
        "#\n",
        "# to the following format:\n",
        "# v7w_2358727.jpg\tWhen was this? Nighttime. \tmatch\n",
        "# v7w_2358727.jpg\tWhen was this?  Daytime. \tno-match\n",
        "# v7w_2358727.jpg\tWhen was this?  Dawn. \tno-match\n",
        "# v7w_2358727.jpg\tWhen was this?  Sunset.\tno-match\n",
        "#\n",
        "# The list above contains the image file name, the question-answer pairs, and the labels.\n",
        "# Only question types \"when\", \"where\" and \"who\" were used due to compute requirements. In\n",
        "# this folder, files v7w.*Images.itm.txt are used and v7w.*Images.txt are ignored. The\n",
        "# two formats are provided for your information and convenience.\n",
        "#\n",
        "# To enable the above this implementation provides the following classes and functions:\n",
        "# - Class ITM_Dataset() to load the multimodal data (image & text (question and answer)).\n",
        "# - Class Transformer_VisionEncoder() to create a pre-trained Vision Transformer, which\n",
        "#   can be finetuned or trained from scratch -- update USE_PRETRAINED_MODEL accordingly.\n",
        "# - Function load_sentence_embeddings() to load pre-generated sentence embeddings of questions\n",
        "#   and answers, which were generated using SentenceTransformer('sentence-transformers/gtr-t5-large').\n",
        "# - Class ITM_Model() to create a model combining the vision and text encoders above.\n",
        "# - Function train_model trains/finetunes one of two possible models: CNN or ViT. The CNN\n",
        "#   model is based on resnet18, and the Vision Transformer (ViT) is based on vit_b_32.\n",
        "# - Function evaluate_model() calculates the accuracy of the selected model using test data.\n",
        "# - The last block of code brings everything together calling all classes & functions above.\n",
        "#\n",
        "# info of resnet18: https://pytorch.org/vision/main/models/resnet.html\n",
        "# info of vit_b_32: https://pytorch.org/vision/main/models/vision_transformer.html\n",
        "# info of SentenceTransformer: https://huggingface.co/sentence-transformers/gtr-t5-large\n",
        "#\n",
        "# This program was tested on Windows 11 using WSL and does not generate any plots.\n",
        "# Feel free to use and extend this program as part of your our assignment work.\n",
        "#\n",
        "# Version 1.0, main functionality in tensorflow tested with COCO data\n",
        "# Version 1.2, extended functionality for Flickr data\n",
        "# Version 1.3, ported to pytorch and tested with visual7w data\n",
        "# Contact: {hcuayahuitl}@lincoln.ac.uk\n",
        "################################################################################\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import pickle\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.models import vit_b_32\n"
      ],
      "metadata": {
        "id": "CJxxnQTfBkgl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Simple code to import google drive folder\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5PZbj5cCa9I",
        "outputId": "b17c17a0-1000-40f8-b04d-253f9809b2d4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # prompt: i imported my google drive folder, i need to navidate to the folder ohis to my zipped dataset call ITM_classifier\n",
        "\n",
        "# # Assuming your zipped dataset is in 'My Drive/ohis'\n",
        "# zip_file_path = '/content/drive/MyDrive/ohis/ITM_Classifier-baselines.zip'\n",
        "\n",
        "# # Check if the zip file exists\n",
        "# if os.path.exists(zip_file_path):\n",
        "#   print(f\"Zip file found at: {zip_file_path}\")\n",
        "\n",
        "# else:\n",
        "#   print(f\"Error: Zip file not found at {zip_file_path}. Please check the path.\")\n"
      ],
      "metadata": {
        "id": "0zGW8u0oCXCi",
        "outputId": "b9f8e640-6ab6-4a0a-fc08-bcbef58d1536",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zip file found at: /content/drive/MyDrive/ohis/ITM_Classifier-baselines.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # prompt: code to unzip my folder - zip_file_path\n",
        "\n",
        "# import zipfile\n",
        "# import os\n",
        "\n",
        "# # Assuming your zipped dataset is in 'My Drive/ohis'\n",
        "# zip_file_path = '/content/drive/MyDrive/ohis/ITM_Classifier-baselines.zip'\n",
        "\n",
        "# # Check if the zip file exists\n",
        "# if os.path.exists(zip_file_path):\n",
        "#   print(f\"Zip file found at: {zip_file_path}\")\n",
        "#   try:\n",
        "#     with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "#       zip_ref.extractall('/content/drive/MyDrive/ohis/') # Extract to the same directory\n",
        "#     print(\"Files extracted successfully!\")\n",
        "\n",
        "#   except zipfile.BadZipFile:\n",
        "#     print(f\"Error: The file at {zip_file_path} is not a valid zip file.\")\n",
        "\n",
        "#   except Exception as e:\n",
        "#     print(f\"An error occurred during extraction: {e}\")\n",
        "# else:\n",
        "#   print(f\"Error: Zip file not found at {zip_file_path}. Please check the path.\")\n"
      ],
      "metadata": {
        "id": "Zn1l7v8TFfMg",
        "outputId": "0814ee7d-8c15-4202-fd70-b8cae1e292a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zip file found at: /content/drive/MyDrive/ohis/ITM_Classifier-baselines.zip\n",
            "Files extracted successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Dataset\n",
        "class ITM_Dataset(Dataset):\n",
        "    def __init__(self, images_path, data_file, sentence_embeddings, data_split, train_ratio=1.0):\n",
        "        self.images_path = images_path\n",
        "        self.data_file = data_file\n",
        "        self.sentence_embeddings = sentence_embeddings\n",
        "        self.data_split = data_split.lower()\n",
        "        self.train_ratio = train_ratio if self.data_split == \"train\" else 1.0\n",
        "\n",
        "        self.image_data = []\n",
        "        self.question_data = []\n",
        "        self.answer_data = []\n",
        "        self.question_embeddings_data = []\n",
        "        self.answer_embeddings_data = []\n",
        "        self.label_data = []\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Standard for pretrained models on ImageNet\n",
        "        ])\n",
        "\n",
        "        self.load_data()"
      ],
      "metadata": {
        "id": "be220gMBBmpj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Dataset\n",
        "class ITM_Dataset(Dataset):\n",
        "    def __init__(self, images_path, data_file, sentence_embeddings, data_split, train_ratio=1.0):\n",
        "        self.images_path = images_path\n",
        "        self.data_file = data_file\n",
        "        self.sentence_embeddings = sentence_embeddings\n",
        "        self.data_split = data_split.lower()\n",
        "        self.train_ratio = train_ratio if self.data_split == \"train\" else 1.0\n",
        "\n",
        "        self.image_data = []\n",
        "        self.question_data = []\n",
        "        self.answer_data = []\n",
        "        self.question_embeddings_data = []\n",
        "        self.answer_embeddings_data = []\n",
        "        self.label_data = []\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Standard for pretrained models on ImageNet\n",
        "        ])\n",
        "\n",
        "        self.load_data()\n",
        "\n",
        "    def load_data(self):\n",
        "        print(\"LOADING data from \"+str(self.data_file))\n",
        "        print(\"=========================================\")\n",
        "\n",
        "        random.seed(42)\n",
        "\n",
        "        with open(self.data_file) as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "            # Apply train_ratio only for training data\n",
        "            if self.data_split == \"train\":\n",
        "                random.shuffle(lines)  # Shuffle before selecting\n",
        "                num_samples = int(len(lines) * self.train_ratio)\n",
        "                lines = lines[:num_samples]\n",
        "\n",
        "            for line in lines:\n",
        "                line = line.rstrip(\"\\n\")\n",
        "                img_name, text, raw_label = line.split(\"\\t\")\n",
        "                img_path = os.path.join(self.images_path, img_name.strip())\n",
        "\n",
        "                question_answer_text = text.split(\"?\")\n",
        "                question_text = question_answer_text[0].strip() + '?'\n",
        "                answer_text = question_answer_text[1].strip()\n",
        "\n",
        "                # Get binary labels from match/no-match answers\n",
        "                label = 1 if raw_label == \"match\" else 0\n",
        "                self.image_data.append(img_path)\n",
        "                self.question_data.append(question_text)\n",
        "                self.answer_data.append(answer_text)\n",
        "                self.question_embeddings_data.append(self.sentence_embeddings[question_text])\n",
        "                self.answer_embeddings_data.append(self.sentence_embeddings[answer_text])\n",
        "                self.label_data.append(label)\n",
        "\n",
        "        print(\"|image_data|=\"+str(len(self.image_data)))\n",
        "        print(\"|question_data|=\"+str(len(self.question_data)))\n",
        "        print(\"|answer_data|=\"+str(len(self.answer_data)))\n",
        "        print(\"done loading data...\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_data[idx]\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        img = self.transform(img)\n",
        "        question_embedding = torch.tensor(self.question_embeddings_data[idx], dtype=torch.float32)\n",
        "        answer_embedding = torch.tensor(self.answer_embeddings_data[idx], dtype=torch.float32)\n",
        "        label = torch.tensor(self.label_data[idx], dtype=torch.long)\n",
        "        return img, question_embedding, answer_embedding, label\n"
      ],
      "metadata": {
        "id": "YdkP0lLzkgIF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load sentence embeddings from an existing file -- generated a priori\n",
        "def load_sentence_embeddings(file_path):\n",
        "    print(\"READING sentence embeddings...\")\n",
        "    with open(file_path, 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "    return data\n",
        "\n",
        "# Pre-trained ViT model\n",
        "class Transformer_VisionEncoder(nn.Module):\n",
        "    def __init__(self, pretrained=None):\n",
        "        super(Transformer_VisionEncoder, self).__init__()\n",
        "\n",
        "        if pretrained:\n",
        "            self.vision_model = vit_b_32(weights=\"IMAGENET1K_V1\")\n",
        "            # Freeze all layers initially\n",
        "            for param in self.vision_model.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "            # Unfreeze the last two layers\n",
        "            for param in list(self.vision_model.heads.parameters())[-2:]:\n",
        "                param.requires_grad = True\n",
        "        else:\n",
        "            self.vision_model = vit_b_32(weights=None)  # Initialize without pretrained weights\n",
        "\n",
        "        # Get feature size after initialising the model\n",
        "        self.num_features = self.vision_model.heads[0].in_features\n",
        "\n",
        "        # Remove original classification head\n",
        "        self.vision_model.heads = nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.vision_model(x)  # Shape should be (batch_size, num_features)\n",
        "        return features\n"
      ],
      "metadata": {
        "id": "U6AXcagpkgEZ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Image-Text Matching Model\n",
        "class ITM_Model(nn.Module):\n",
        "    def __init__(self, num_classes=2, ARCHITECTURE=None, PRETRAINED=None):\n",
        "        print(f'BUILDING %s model, pretrained=%s' % (ARCHITECTURE, PRETRAINED))\n",
        "        super(ITM_Model, self).__init__()\n",
        "        self.ARCHITECTURE = ARCHITECTURE\n",
        "\n",
        "        if self.ARCHITECTURE == \"CNN\":\n",
        "            self.vision_model = models.resnet18(pretrained=PRETRAINED)\n",
        "            if PRETRAINED:\n",
        "\t\t\t    # Freeze all layers\n",
        "                for param in self.vision_model.parameters():\n",
        "                    param.requires_grad = False\n",
        "                # Unfreeze the last two layers\n",
        "                for param in list(self.vision_model.children())[-2:]:\n",
        "                    for p in param.parameters():\n",
        "                        p.requires_grad = True\n",
        "            else:\n",
        "                for param in self.vision_model.parameters():\n",
        "                    param.requires_grad = True\n",
        "            self.vision_model.fc = nn.Linear(self.vision_model.fc.in_features, 128) # Change output\n",
        "\n",
        "        elif self.ARCHITECTURE == \"ViT\":\n",
        "            self.vision_model = Transformer_VisionEncoder(pretrained=PRETRAINED)\n",
        "            self.fc_vit = nn.Linear(self.vision_model.num_features, 128)  # Reduce features\n",
        "\n",
        "        else:\n",
        "            print(\"UNKNOWN neural architecture\", ARCHITECTURE)\n",
        "            exit(0)\n",
        "\n",
        "        self.question_embedding_layer = nn.Linear(768, 128)  # Adjust question dimension\n",
        "        self.answer_embedding_layer = nn.Linear(768, 128)  # Adjust answer dimension\n",
        "        self.fc = nn.Linear(128 + 128 + 128, num_classes)  # Concatenate vision and text features\n",
        "\n",
        "    def forward(self, img, question_embedding, answer_embedding):\n",
        "        img_features = self.vision_model(img)\n",
        "        if self.ARCHITECTURE == \"ViT\":\n",
        "            img_features = self.fc_vit(img_features) # Use the custom linear layer for ViT\n",
        "        question_features = self.question_embedding_layer(question_embedding)\n",
        "        answer_features = self.answer_embedding_layer(answer_embedding)\n",
        "        combined_features = torch.cat((img_features, question_features, answer_features), dim=1)\n",
        "        output = self.fc(combined_features)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "uXlHvbrQkgBR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, ARCHITECTURE, train_loader, criterion, optimiser, num_epochs=10):\n",
        "    print(f'TRAINING %s model' % (ARCHITECTURE))\n",
        "    model.train()\n",
        "\n",
        "    # Track the overall loss for each epoch\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        total_batches = len(train_loader)\n",
        "        start_time = time.time()\n",
        "\n",
        "        for batch_idx, (images, question_embeddings, answer_embeddings, labels) in enumerate(train_loader):\n",
        "            # Move images/text/labels to the GPU (if available)\n",
        "            images = images.to(device)\n",
        "            question_embeddings = question_embeddings.to(device)\n",
        "            answer_embeddings = answer_embeddings.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass -- given input data to the model\n",
        "            outputs = model(images, question_embeddings, answer_embeddings)\n",
        "\n",
        "            # Calculate loss (error)\n",
        "            loss = criterion(outputs, labels)  # output should be raw logits\n",
        "\n",
        "            # Backward pass -- given loss above\n",
        "            optimiser.zero_grad() # clear the gradients\n",
        "            loss.backward() # computes gradient of the loss/error\n",
        "            optimiser.step() # updates parameters using gradients\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Print progress every X batches\n",
        "            if batch_idx % 100 == 0:\n",
        "                print(f'Epoch [{epoch + 1}/{num_epochs}], Batch [{batch_idx}/{total_batches}], Loss: {loss.item():.4f}')\n",
        "\n",
        "        # Print average loss for the epoch\n",
        "        avg_loss = running_loss / total_batches\n",
        "        elapsed_time = time.time() - start_time\n",
        "        print(f'Epoch [{epoch + 1}/{num_epochs}] Average Loss: {avg_loss:.4f}, {elapsed_time:.2f} seconds')\n"
      ],
      "metadata": {
        "id": "3eN2aSkGkf_F"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, ARCHITECTURE, test_loader, device):\n",
        "    print(f'EVALUATING %s model' % (ARCHITECTURE))\n",
        "    model.eval()\n",
        "    total_test_loss = 0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    start_time = time.time()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, question_embeddings, answer_embeddings, labels in test_loader:\n",
        "            # Move images/text/labels to the GPU (if available)\n",
        "            images = images.to(device)\n",
        "            question_embeddings = question_embeddings.to(device)\n",
        "            answer_embeddings = answer_embeddings.to(device)\n",
        "            labels = labels.to(device)  # Labels are single integers (0 or 1)\n",
        "\n",
        "            # Perform forward pass on our data\n",
        "            outputs = model(images, question_embeddings, answer_embeddings)\n",
        "\n",
        "            # Accumulate loss on test data\n",
        "            total_test_loss += criterion(outputs, labels)\n",
        "\n",
        "            # Since outputs are logits, apply softmax to get probabilities\n",
        "            predicted_probabilities = torch.softmax(outputs, dim=1)  # Use softmax for multi-class output\n",
        "            predicted_class = predicted_probabilities.argmax(dim=1)  # Get the predicted class index (0 or 1)\n",
        "\n",
        "            # Store labels and predictions for later analysis\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted_class.cpu().numpy())\n",
        "\n",
        "    # Convert to numpy arrays for easier calculations\n",
        "    all_labels = np.array(all_labels)\n",
        "    all_predictions = np.array(all_predictions)\n",
        "\n",
        "    # Calculate true positives, true negatives, false positives, false negatives\n",
        "    tp = np.sum((all_predictions == 1) & (all_labels == 1))  # True positives\n",
        "    tn = np.sum((all_predictions == 0) & (all_labels == 0))  # True negatives\n",
        "    fp = np.sum((all_predictions == 1) & (all_labels == 0))  # False positives\n",
        "    fn = np.sum((all_predictions == 0) & (all_labels == 1))  # False negatives\n",
        "\n",
        "    # Calculate sensitivity, specificity, and balanced accuracy\n",
        "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
        "    balanced_accuracy = (sensitivity + specificity) / 2.0\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f'Balanced Accuracy: {balanced_accuracy:.4f}, {elapsed_time:.2f} seconds')\n",
        "    print(f'Total Test Loss: {total_test_loss:.4f}')\n",
        "\n"
      ],
      "metadata": {
        "id": "UKUcjEY_ksyA"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main Execution\n",
        "if __name__ == '__main__':\n",
        "    # Check GPU availability\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f'Using device: {device}')\n",
        "\n",
        "    # Paths and files\n",
        "    # Paths and files\n",
        "    IMAGES_PATH = \"/content/drive/MyDrive/ohis/visual7w-images\"\n",
        "    train_data_file = \"/content/drive/MyDrive/ohis/visual7w-text/v7w.TrainImages.itm.txt\"\n",
        "    dev_data_file = \"/content/drive/MyDrive/ohis/visual7w-text/v7w.DevImages.itm.txt\"\n",
        "    test_data_file = \"/content/drive/MyDrive/ohis/visual7w-text/v7w.TestImages.itm.txt\"\n",
        "    sentence_embeddings_file = \"/content/drive/MyDrive/ohis/v7w.sentence_embeddings-gtr-t5-large.pkl\"\n",
        "    sentence_embeddings = load_sentence_embeddings(sentence_embeddings_file)\n",
        "\n",
        "    # Create datasets and loaders\n",
        "    train_dataset = ITM_Dataset(IMAGES_PATH, train_data_file, sentence_embeddings, data_split=\"train\", train_ratio=0.2)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "    test_dataset = ITM_Dataset(IMAGES_PATH, test_data_file, sentence_embeddings, data_split=\"test\")  # whole test data\n",
        "    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "    # The dev set is not used in this program and you should/could use it for example to optimise your hyperparameters\n",
        "    #dev_dataset = ITM_Dataset(images_path, \"dev_data.txt\", sentence_embeddings, data_split=\"dev\")  # whole dev data\n",
        "\n",
        "    # Create the model using one of the two supported architectures\n",
        "    MODEL_ARCHITECTURE = \"CNN\" # options are \"CNN\" or \"ViT\"\n",
        "    USE_PRETRAINED_MODEL = True\n",
        "    model = ITM_Model(num_classes=2, ARCHITECTURE=MODEL_ARCHITECTURE, PRETRAINED=USE_PRETRAINED_MODEL).to(device)\n",
        "    print(\"\\nModel Architecture:\")\n",
        "    print(model)\n",
        "\n",
        "    # Print the parameters of the model selected above\n",
        "    total_params = 0\n",
        "    print(\"\\nModel Trainable Parameters:\")\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:  # print trainable parameters\n",
        "            num_params = param.numel()\n",
        "            total_params += num_params\n",
        "            print(f\"{name}: {param.data.shape} | Number of parameters: {num_params}\")\n",
        "    print(f\"\\nTotal number of parameters in the model: {total_params}\")\n",
        "    print(f\"\\nUSE_PRETRAINED_MODEL={USE_PRETRAINED_MODEL}\\n\")\n",
        "\n",
        "    # Define loss function and optimiser\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimiser = torch.optim.AdamW(model.parameters(), lr=3e-5, weight_decay=1e-4)\n",
        "\n",
        "    # Train and evaluate the model\n",
        "    train_model(model, MODEL_ARCHITECTURE, train_loader, criterion, optimiser, num_epochs=10)\n",
        "    evaluate_model(model, MODEL_ARCHITECTURE, test_loader, device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2UO-l9jksvZ",
        "outputId": "9e63655f-5abb-417a-f40a-caefe1614c91"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "READING sentence embeddings...\n",
            "LOADING data from /content/drive/MyDrive/ohis/visual7w-text/v7w.TrainImages.itm.txt\n",
            "=========================================\n",
            "|image_data|=9780\n",
            "|question_data|=9780\n",
            "|answer_data|=9780\n",
            "done loading data...\n",
            "LOADING data from /content/drive/MyDrive/ohis/visual7w-text/v7w.TestImages.itm.txt\n",
            "=========================================\n",
            "|image_data|=5980\n",
            "|question_data|=5980\n",
            "|answer_data|=5980\n",
            "done loading data...\n",
            "BUILDING CNN model, pretrained=True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 184MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Architecture:\n",
            "ITM_Model(\n",
            "  (vision_model): ResNet(\n",
            "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): Linear(in_features=512, out_features=128, bias=True)\n",
            "  )\n",
            "  (question_embedding_layer): Linear(in_features=768, out_features=128, bias=True)\n",
            "  (answer_embedding_layer): Linear(in_features=768, out_features=128, bias=True)\n",
            "  (fc): Linear(in_features=384, out_features=2, bias=True)\n",
            ")\n",
            "\n",
            "Model Trainable Parameters:\n",
            "vision_model.fc.weight: torch.Size([128, 512]) | Number of parameters: 65536\n",
            "vision_model.fc.bias: torch.Size([128]) | Number of parameters: 128\n",
            "question_embedding_layer.weight: torch.Size([128, 768]) | Number of parameters: 98304\n",
            "question_embedding_layer.bias: torch.Size([128]) | Number of parameters: 128\n",
            "answer_embedding_layer.weight: torch.Size([128, 768]) | Number of parameters: 98304\n",
            "answer_embedding_layer.bias: torch.Size([128]) | Number of parameters: 128\n",
            "fc.weight: torch.Size([2, 384]) | Number of parameters: 768\n",
            "fc.bias: torch.Size([2]) | Number of parameters: 2\n",
            "\n",
            "Total number of parameters in the model: 263298\n",
            "\n",
            "USE_PRETRAINED_MODEL=True\n",
            "\n",
            "TRAINING CNN model\n",
            "Epoch [1/10], Batch [0/612], Loss: 0.6172\n",
            "Epoch [1/10], Batch [100/612], Loss: 0.6068\n",
            "Epoch [1/10], Batch [200/612], Loss: 0.4932\n",
            "Epoch [1/10], Batch [300/612], Loss: 0.7684\n",
            "Epoch [1/10], Batch [400/612], Loss: 0.5676\n",
            "Epoch [1/10], Batch [500/612], Loss: 0.6350\n",
            "Epoch [1/10], Batch [600/612], Loss: 0.5528\n",
            "Epoch [1/10] Average Loss: 0.5638, 55.94 seconds\n",
            "Epoch [2/10], Batch [0/612], Loss: 0.6920\n",
            "Epoch [2/10], Batch [100/612], Loss: 0.4004\n",
            "Epoch [2/10], Batch [200/612], Loss: 0.3384\n",
            "Epoch [2/10], Batch [300/612], Loss: 0.6989\n",
            "Epoch [2/10], Batch [400/612], Loss: 0.3514\n",
            "Epoch [2/10], Batch [500/612], Loss: 0.5347\n",
            "Epoch [2/10], Batch [600/612], Loss: 0.4937\n",
            "Epoch [2/10] Average Loss: 0.5443, 53.13 seconds\n",
            "Epoch [3/10], Batch [0/612], Loss: 0.3800\n",
            "Epoch [3/10], Batch [100/612], Loss: 0.4895\n",
            "Epoch [3/10], Batch [200/612], Loss: 0.8370\n",
            "Epoch [3/10], Batch [300/612], Loss: 0.5141\n",
            "Epoch [3/10], Batch [400/612], Loss: 0.7305\n",
            "Epoch [3/10], Batch [500/612], Loss: 0.4152\n",
            "Epoch [3/10], Batch [600/612], Loss: 0.4626\n",
            "Epoch [3/10] Average Loss: 0.5164, 53.19 seconds\n",
            "Epoch [4/10], Batch [0/612], Loss: 0.4412\n",
            "Epoch [4/10], Batch [100/612], Loss: 0.4214\n",
            "Epoch [4/10], Batch [200/612], Loss: 0.4380\n",
            "Epoch [4/10], Batch [300/612], Loss: 0.5634\n",
            "Epoch [4/10], Batch [400/612], Loss: 0.6135\n",
            "Epoch [4/10], Batch [500/612], Loss: 0.4446\n",
            "Epoch [4/10], Batch [600/612], Loss: 0.6897\n",
            "Epoch [4/10] Average Loss: 0.4935, 52.65 seconds\n",
            "Epoch [5/10], Batch [0/612], Loss: 0.5521\n",
            "Epoch [5/10], Batch [100/612], Loss: 0.5852\n",
            "Epoch [5/10], Batch [200/612], Loss: 0.3205\n",
            "Epoch [5/10], Batch [300/612], Loss: 0.5241\n",
            "Epoch [5/10], Batch [400/612], Loss: 0.4122\n",
            "Epoch [5/10], Batch [500/612], Loss: 0.5748\n",
            "Epoch [5/10], Batch [600/612], Loss: 0.4782\n",
            "Epoch [5/10] Average Loss: 0.4785, 53.62 seconds\n",
            "Epoch [6/10], Batch [0/612], Loss: 0.4197\n",
            "Epoch [6/10], Batch [100/612], Loss: 0.4313\n",
            "Epoch [6/10], Batch [200/612], Loss: 0.5728\n",
            "Epoch [6/10], Batch [300/612], Loss: 0.6206\n",
            "Epoch [6/10], Batch [400/612], Loss: 0.3761\n",
            "Epoch [6/10], Batch [500/612], Loss: 0.2828\n",
            "Epoch [6/10], Batch [600/612], Loss: 0.4229\n",
            "Epoch [6/10] Average Loss: 0.4670, 53.35 seconds\n",
            "Epoch [7/10], Batch [0/612], Loss: 0.2966\n",
            "Epoch [7/10], Batch [100/612], Loss: 0.4446\n",
            "Epoch [7/10], Batch [200/612], Loss: 0.4788\n",
            "Epoch [7/10], Batch [300/612], Loss: 0.5209\n",
            "Epoch [7/10], Batch [400/612], Loss: 0.3252\n",
            "Epoch [7/10], Batch [500/612], Loss: 0.4724\n",
            "Epoch [7/10], Batch [600/612], Loss: 0.3369\n",
            "Epoch [7/10] Average Loss: 0.4596, 53.17 seconds\n",
            "Epoch [8/10], Batch [0/612], Loss: 0.5022\n",
            "Epoch [8/10], Batch [100/612], Loss: 0.4289\n",
            "Epoch [8/10], Batch [200/612], Loss: 0.5566\n",
            "Epoch [8/10], Batch [300/612], Loss: 0.3194\n",
            "Epoch [8/10], Batch [400/612], Loss: 0.4417\n",
            "Epoch [8/10], Batch [500/612], Loss: 0.4331\n",
            "Epoch [8/10], Batch [600/612], Loss: 0.4510\n",
            "Epoch [8/10] Average Loss: 0.4540, 52.73 seconds\n",
            "Epoch [9/10], Batch [0/612], Loss: 0.4788\n",
            "Epoch [9/10], Batch [100/612], Loss: 0.2826\n",
            "Epoch [9/10], Batch [200/612], Loss: 0.7691\n",
            "Epoch [9/10], Batch [300/612], Loss: 0.3024\n",
            "Epoch [9/10], Batch [400/612], Loss: 0.2915\n",
            "Epoch [9/10], Batch [500/612], Loss: 0.4152\n",
            "Epoch [9/10], Batch [600/612], Loss: 0.2465\n",
            "Epoch [9/10] Average Loss: 0.4495, 52.89 seconds\n",
            "Epoch [10/10], Batch [0/612], Loss: 0.4804\n",
            "Epoch [10/10], Batch [100/612], Loss: 0.6223\n",
            "Epoch [10/10], Batch [200/612], Loss: 0.3283\n",
            "Epoch [10/10], Batch [300/612], Loss: 0.5094\n",
            "Epoch [10/10], Batch [400/612], Loss: 0.6688\n",
            "Epoch [10/10], Batch [500/612], Loss: 0.4099\n",
            "Epoch [10/10], Batch [600/612], Loss: 0.4039\n",
            "Epoch [10/10] Average Loss: 0.4459, 52.81 seconds\n",
            "EVALUATING CNN model\n",
            "Balanced Accuracy: 0.6307, 31.01 seconds\n",
            "Total Test Loss: 172.3385\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-O0kA1gSkssa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GzvM2qdVkf7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8YVEsk6akf5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-5tfX5wSkf2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g-HylSDUkfzM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}