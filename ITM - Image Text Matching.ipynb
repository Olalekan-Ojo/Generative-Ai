{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Olalekan-Ojo/Generative-Ai/blob/main/ITM%20-%20Image%20Text%20Matching.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fYMEeyo9BOdP"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# Image-Text Matching Classifier: baseline system for visual question answering\n",
        "#\n",
        "# This program has been adapted and rewriten from the CMP9137 materials of 2024.\n",
        "#\n",
        "# It treats the task of multi-choice visual question answering as a binary\n",
        "# classification task. This is possible by rewriting the questions from this format:\n",
        "# v7w_2358727.jpg\tWhen was this?  Nighttime. | Daytime. | Dawn. Sunset.\n",
        "#\n",
        "# to the following format:\n",
        "# v7w_2358727.jpg\tWhen was this? Nighttime. \tmatch\n",
        "# v7w_2358727.jpg\tWhen was this?  Daytime. \tno-match\n",
        "# v7w_2358727.jpg\tWhen was this?  Dawn. \tno-match\n",
        "# v7w_2358727.jpg\tWhen was this?  Sunset.\tno-match\n",
        "#\n",
        "# The list above contains the image file name, the question-answer pairs, and the labels.\n",
        "# Only question types \"when\", \"where\" and \"who\" were used due to compute requirements. In\n",
        "# this folder, files v7w.*Images.itm.txt are used and v7w.*Images.txt are ignored. The\n",
        "# two formats are provided for your information and convenience.\n",
        "#\n",
        "# To enable the above this implementation provides the following classes and functions:\n",
        "# - Class ITM_Dataset() to load the multimodal data (image & text (question and answer)).\n",
        "# - Class Transformer_VisionEncoder() to create a pre-trained Vision Transformer, which\n",
        "#   can be finetuned or trained from scratch -- update USE_PRETRAINED_MODEL accordingly.\n",
        "# - Function load_sentence_embeddings() to load pre-generated sentence embeddings of questions\n",
        "#   and answers, which were generated using SentenceTransformer('sentence-transformers/gtr-t5-large').\n",
        "# - Class ITM_Model() to create a model combining the vision and text encoders above.\n",
        "# - Function train_model trains/finetunes one of two possible models: CNN or ViT. The CNN\n",
        "#   model is based on resnet18, and the Vision Transformer (ViT) is based on vit_b_32.\n",
        "# - Function evaluate_model() calculates the accuracy of the selected model using test data.\n",
        "# - The last block of code brings everything together calling all classes & functions above.\n",
        "#\n",
        "# info of resnet18: https://pytorch.org/vision/main/models/resnet.html\n",
        "# info of vit_b_32: https://pytorch.org/vision/main/models/vision_transformer.html\n",
        "# info of SentenceTransformer: https://huggingface.co/sentence-transformers/gtr-t5-large\n",
        "#\n",
        "# This program was tested on Windows 11 using WSL and does not generate any plots.\n",
        "# Feel free to use and extend this program as part of your our assignment work.\n",
        "#\n",
        "# Version 1.0, main functionality in tensorflow tested with COCO data\n",
        "# Version 1.2, extended functionality for Flickr data\n",
        "# Version 1.3, ported to pytorch and tested with visual7w data\n",
        "# Contact: {hcuayahuitl}@lincoln.ac.uk\n",
        "################################################################################\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import pickle\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.models import vit_b_32\n"
      ],
      "metadata": {
        "id": "CJxxnQTfBkgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Simple code to import google drive folder\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5PZbj5cCa9I",
        "outputId": "ba426b79-e8f3-4a0d-89ba-daade45ef184"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # prompt: i imported my google drive folder, i need to navidate to the folder ohis to my zipped dataset call ITM_classifier\n",
        "\n",
        "# # Assuming your zipped dataset is in 'My Drive/ohis'\n",
        "# zip_file_path = '/content/drive/MyDrive/ohis/ITM_Classifier-baselines.zip'\n",
        "\n",
        "# # Check if the zip file exists\n",
        "# if os.path.exists(zip_file_path):\n",
        "#   print(f\"Zip file found at: {zip_file_path}\")\n",
        "\n",
        "# else:\n",
        "#   print(f\"Error: Zip file not found at {zip_file_path}. Please check the path.\")\n"
      ],
      "metadata": {
        "id": "0zGW8u0oCXCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # prompt: code to unzip my folder - zip_file_path\n",
        "\n",
        "# import zipfile\n",
        "# import os\n",
        "\n",
        "# # Assuming your zipped dataset is in 'My Drive/ohis'\n",
        "# zip_file_path = '/content/drive/MyDrive/ohis/ITM_Classifier-baselines.zip'\n",
        "\n",
        "# # Check if the zip file exists\n",
        "# if os.path.exists(zip_file_path):\n",
        "#   print(f\"Zip file found at: {zip_file_path}\")\n",
        "#   try:\n",
        "#     with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "#       zip_ref.extractall('/content/drive/MyDrive/ohis/') # Extract to the same directory\n",
        "#     print(\"Files extracted successfully!\")\n",
        "\n",
        "#   except zipfile.BadZipFile:\n",
        "#     print(f\"Error: The file at {zip_file_path} is not a valid zip file.\")\n",
        "\n",
        "#   except Exception as e:\n",
        "#     print(f\"An error occurred during extraction: {e}\")\n",
        "# else:\n",
        "#   print(f\"Error: Zip file not found at {zip_file_path}. Please check the path.\")\n"
      ],
      "metadata": {
        "id": "Zn1l7v8TFfMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Dataset\n",
        "class ITM_Dataset(Dataset):\n",
        "    def __init__(self, images_path, data_file, sentence_embeddings, data_split, train_ratio=1.0):\n",
        "        self.images_path = images_path\n",
        "        self.data_file = data_file\n",
        "        self.sentence_embeddings = sentence_embeddings\n",
        "        self.data_split = data_split.lower()\n",
        "        self.train_ratio = train_ratio if self.data_split == \"train\" else 1.0\n",
        "\n",
        "        self.image_data = []\n",
        "        self.question_data = []\n",
        "        self.answer_data = []\n",
        "        self.question_embeddings_data = []\n",
        "        self.answer_embeddings_data = []\n",
        "        self.label_data = []\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Standard for pretrained models on ImageNet\n",
        "        ])\n",
        "\n",
        "        self.load_data()"
      ],
      "metadata": {
        "id": "be220gMBBmpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Dataset\n",
        "class ITM_Dataset(Dataset):\n",
        "    def __init__(self, images_path, data_file, sentence_embeddings, data_split, train_ratio=1.0):\n",
        "        self.images_path = images_path\n",
        "        self.data_file = data_file\n",
        "        self.sentence_embeddings = sentence_embeddings\n",
        "        self.data_split = data_split.lower()\n",
        "        self.train_ratio = train_ratio if self.data_split == \"train\" else 1.0\n",
        "\n",
        "        self.image_data = []\n",
        "        self.question_data = []\n",
        "        self.answer_data = []\n",
        "        self.question_embeddings_data = []\n",
        "        self.answer_embeddings_data = []\n",
        "        self.label_data = []\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Standard for pretrained models on ImageNet\n",
        "        ])\n",
        "\n",
        "        self.load_data()\n",
        "\n",
        "    def load_data(self):\n",
        "        print(\"LOADING data from \"+str(self.data_file))\n",
        "        print(\"=========================================\")\n",
        "\n",
        "        random.seed(42)\n",
        "\n",
        "        with open(self.data_file) as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "            # Apply train_ratio only for training data\n",
        "            if self.data_split == \"train\":\n",
        "                random.shuffle(lines)  # Shuffle before selecting\n",
        "                num_samples = int(len(lines) * self.train_ratio)\n",
        "                lines = lines[:num_samples]\n",
        "\n",
        "            for line in lines:\n",
        "                line = line.rstrip(\"\\n\")\n",
        "                img_name, text, raw_label = line.split(\"\\t\")\n",
        "                img_path = os.path.join(self.images_path, img_name.strip())\n",
        "\n",
        "                question_answer_text = text.split(\"?\")\n",
        "                question_text = question_answer_text[0].strip() + '?'\n",
        "                answer_text = question_answer_text[1].strip()\n",
        "\n",
        "                # Get binary labels from match/no-match answers\n",
        "                label = 1 if raw_label == \"match\" else 0\n",
        "                self.image_data.append(img_path)\n",
        "                self.question_data.append(question_text)\n",
        "                self.answer_data.append(answer_text)\n",
        "                self.question_embeddings_data.append(self.sentence_embeddings[question_text])\n",
        "                self.answer_embeddings_data.append(self.sentence_embeddings[answer_text])\n",
        "                self.label_data.append(label)\n",
        "\n",
        "        print(\"|image_data|=\"+str(len(self.image_data)))\n",
        "        print(\"|question_data|=\"+str(len(self.question_data)))\n",
        "        print(\"|answer_data|=\"+str(len(self.answer_data)))\n",
        "        print(\"done loading data...\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_data[idx]\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        img = self.transform(img)\n",
        "        question_embedding = torch.tensor(self.question_embeddings_data[idx], dtype=torch.float32)\n",
        "        answer_embedding = torch.tensor(self.answer_embeddings_data[idx], dtype=torch.float32)\n",
        "        label = torch.tensor(self.label_data[idx], dtype=torch.long)\n",
        "        return img, question_embedding, answer_embedding, label\n"
      ],
      "metadata": {
        "id": "YdkP0lLzkgIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load sentence embeddings from an existing file -- generated a priori\n",
        "def load_sentence_embeddings(file_path):\n",
        "    print(\"READING sentence embeddings...\")\n",
        "    with open(file_path, 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "    return data\n",
        "\n",
        "# Pre-trained ViT model\n",
        "class Transformer_VisionEncoder(nn.Module):\n",
        "    def __init__(self, pretrained=None):\n",
        "        super(Transformer_VisionEncoder, self).__init__()\n",
        "\n",
        "        if pretrained:\n",
        "            self.vision_model = vit_b_32(weights=\"IMAGENET1K_V1\")\n",
        "            # Freeze all layers initially\n",
        "            for param in self.vision_model.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "            # Unfreeze the last two layers\n",
        "            for param in list(self.vision_model.heads.parameters())[-2:]:\n",
        "                param.requires_grad = True\n",
        "        else:\n",
        "            self.vision_model = vit_b_32(weights=None)  # Initialize without pretrained weights\n",
        "\n",
        "        # Get feature size after initialising the model\n",
        "        self.num_features = self.vision_model.heads[0].in_features\n",
        "\n",
        "        # Remove original classification head\n",
        "        self.vision_model.heads = nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.vision_model(x)  # Shape should be (batch_size, num_features)\n",
        "        return features\n"
      ],
      "metadata": {
        "id": "U6AXcagpkgEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Image-Text Matching Model\n",
        "class ITM_Model(nn.Module):\n",
        "    def __init__(self, num_classes=2, ARCHITECTURE=None, PRETRAINED=None):\n",
        "        print(f'BUILDING %s model, pretrained=%s' % (ARCHITECTURE, PRETRAINED))\n",
        "        super(ITM_Model, self).__init__()\n",
        "        self.ARCHITECTURE = ARCHITECTURE\n",
        "\n",
        "        if self.ARCHITECTURE == \"CNN\":\n",
        "            self.vision_model = models.resnet18(pretrained=PRETRAINED)\n",
        "            if PRETRAINED:\n",
        "\t\t\t    # Freeze all layers\n",
        "                for param in self.vision_model.parameters():\n",
        "                    param.requires_grad = False\n",
        "                # Unfreeze the last two layers\n",
        "                for param in list(self.vision_model.children())[-2:]:\n",
        "                    for p in param.parameters():\n",
        "                        p.requires_grad = True\n",
        "            else:\n",
        "                for param in self.vision_model.parameters():\n",
        "                    param.requires_grad = True\n",
        "            self.vision_model.fc = nn.Linear(self.vision_model.fc.in_features, 128) # Change output\n",
        "\n",
        "        elif self.ARCHITECTURE == \"ViT\":\n",
        "            self.vision_model = Transformer_VisionEncoder(pretrained=PRETRAINED)\n",
        "            self.fc_vit = nn.Linear(self.vision_model.num_features, 128)  # Reduce features\n",
        "\n",
        "        else:\n",
        "            print(\"UNKNOWN neural architecture\", ARCHITECTURE)\n",
        "            exit(0)\n",
        "\n",
        "        self.question_embedding_layer = nn.Linear(768, 128)  # Adjust question dimension\n",
        "        self.answer_embedding_layer = nn.Linear(768, 128)  # Adjust answer dimension\n",
        "        self.fc = nn.Linear(128 + 128 + 128, num_classes)  # Concatenate vision and text features\n",
        "\n",
        "    def forward(self, img, question_embedding, answer_embedding):\n",
        "        img_features = self.vision_model(img)\n",
        "        if self.ARCHITECTURE == \"ViT\":\n",
        "            img_features = self.fc_vit(img_features) # Use the custom linear layer for ViT\n",
        "        question_features = self.question_embedding_layer(question_embedding)\n",
        "        answer_features = self.answer_embedding_layer(answer_embedding)\n",
        "        combined_features = torch.cat((img_features, question_features, answer_features), dim=1)\n",
        "        output = self.fc(combined_features)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "uXlHvbrQkgBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, ARCHITECTURE, train_loader, criterion, optimiser, num_epochs=10):\n",
        "    print(f'TRAINING %s model' % (ARCHITECTURE))\n",
        "    model.train()\n",
        "\n",
        "    # Track the overall loss for each epoch\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        total_batches = len(train_loader)\n",
        "        start_time = time.time()\n",
        "\n",
        "        for batch_idx, (images, question_embeddings, answer_embeddings, labels) in enumerate(train_loader):\n",
        "            # Move images/text/labels to the GPU (if available)\n",
        "            images = images.to(device)\n",
        "            question_embeddings = question_embeddings.to(device)\n",
        "            answer_embeddings = answer_embeddings.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass -- given input data to the model\n",
        "            outputs = model(images, question_embeddings, answer_embeddings)\n",
        "\n",
        "            # Calculate loss (error)\n",
        "            loss = criterion(outputs, labels)  # output should be raw logits\n",
        "\n",
        "            # Backward pass -- given loss above\n",
        "            optimiser.zero_grad() # clear the gradients\n",
        "            loss.backward() # computes gradient of the loss/error\n",
        "            optimiser.step() # updates parameters using gradients\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Print progress every X batches\n",
        "            if batch_idx % 100 == 0:\n",
        "                print(f'Epoch [{epoch + 1}/{num_epochs}], Batch [{batch_idx}/{total_batches}], Loss: {loss.item():.4f}')\n",
        "\n",
        "        # Print average loss for the epoch\n",
        "        avg_loss = running_loss / total_batches\n",
        "        elapsed_time = time.time() - start_time\n",
        "        print(f'Epoch [{epoch + 1}/{num_epochs}] Average Loss: {avg_loss:.4f}, {elapsed_time:.2f} seconds')\n"
      ],
      "metadata": {
        "id": "3eN2aSkGkf_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, ARCHITECTURE, test_loader, device):\n",
        "    print(f'EVALUATING %s model' % (ARCHITECTURE))\n",
        "    model.eval()\n",
        "    total_test_loss = 0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    start_time = time.time()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, question_embeddings, answer_embeddings, labels in test_loader:\n",
        "            # Move images/text/labels to the GPU (if available)\n",
        "            images = images.to(device)\n",
        "            question_embeddings = question_embeddings.to(device)\n",
        "            answer_embeddings = answer_embeddings.to(device)\n",
        "            labels = labels.to(device)  # Labels are single integers (0 or 1)\n",
        "\n",
        "            # Perform forward pass on our data\n",
        "            outputs = model(images, question_embeddings, answer_embeddings)\n",
        "\n",
        "            # Accumulate loss on test data\n",
        "            total_test_loss += criterion(outputs, labels)\n",
        "\n",
        "            # Since outputs are logits, apply softmax to get probabilities\n",
        "            predicted_probabilities = torch.softmax(outputs, dim=1)  # Use softmax for multi-class output\n",
        "            predicted_class = predicted_probabilities.argmax(dim=1)  # Get the predicted class index (0 or 1)\n",
        "\n",
        "            # Store labels and predictions for later analysis\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted_class.cpu().numpy())\n",
        "\n",
        "    # Convert to numpy arrays for easier calculations\n",
        "    all_labels = np.array(all_labels)\n",
        "    all_predictions = np.array(all_predictions)\n",
        "\n",
        "    # Calculate true positives, true negatives, false positives, false negatives\n",
        "    tp = np.sum((all_predictions == 1) & (all_labels == 1))  # True positives\n",
        "    tn = np.sum((all_predictions == 0) & (all_labels == 0))  # True negatives\n",
        "    fp = np.sum((all_predictions == 1) & (all_labels == 0))  # False positives\n",
        "    fn = np.sum((all_predictions == 0) & (all_labels == 1))  # False negatives\n",
        "\n",
        "    # Calculate sensitivity, specificity, and balanced accuracy\n",
        "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
        "    balanced_accuracy = (sensitivity + specificity) / 2.0\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f'Balanced Accuracy: {balanced_accuracy:.4f}, {elapsed_time:.2f} seconds')\n",
        "    print(f'Total Test Loss: {total_test_loss:.4f}')\n",
        "\n"
      ],
      "metadata": {
        "id": "UKUcjEY_ksyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main Execution\n",
        "if __name__ == '__main__':\n",
        "    # Check GPU availability\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f'Using device: {device}')\n",
        "\n",
        "    # Paths and files\n",
        "    # Paths and files\n",
        "    IMAGES_PATH = \"/content/drive/MyDrive/ohis/visual7w-images\"\n",
        "    train_data_file = \"/content/drive/MyDrive/ohis/visual7w-text/v7w.TrainImages.itm.txt\"\n",
        "    dev_data_file = \"/content/drive/MyDrive/ohis/visual7w-text/v7w.DevImages.itm.txt\"\n",
        "    test_data_file = \"/content/drive/MyDrive/ohis/visual7w-text/v7w.TestImages.itm.txt\"\n",
        "    sentence_embeddings_file = \"/content/drive/MyDrive/ohis/v7w.sentence_embeddings-gtr-t5-large.pkl\"\n",
        "    sentence_embeddings = load_sentence_embeddings(sentence_embeddings_file)\n",
        "\n",
        "    # Create datasets and loaders\n",
        "    train_dataset = ITM_Dataset(IMAGES_PATH, train_data_file, sentence_embeddings, data_split=\"train\", train_ratio=0.2)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "    test_dataset = ITM_Dataset(IMAGES_PATH, test_data_file, sentence_embeddings, data_split=\"test\")  # whole test data\n",
        "    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "    # The dev set is not used in this program and you should/could use it for example to optimise your hyperparameters\n",
        "    #dev_dataset = ITM_Dataset(images_path, \"dev_data.txt\", sentence_embeddings, data_split=\"dev\")  # whole dev data\n",
        "\n",
        "    # Create the model using one of the two supported architectures\n",
        "    MODEL_ARCHITECTURE = \"CNN\" # options are \"CNN\" or \"ViT\"\n",
        "    USE_PRETRAINED_MODEL = True\n",
        "    model = ITM_Model(num_classes=2, ARCHITECTURE=MODEL_ARCHITECTURE, PRETRAINED=USE_PRETRAINED_MODEL).to(device)\n",
        "    print(\"\\nModel Architecture:\")\n",
        "    print(model)\n",
        "\n",
        "    # Print the parameters of the model selected above\n",
        "    total_params = 0\n",
        "    print(\"\\nModel Trainable Parameters:\")\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:  # print trainable parameters\n",
        "            num_params = param.numel()\n",
        "            total_params += num_params\n",
        "            print(f\"{name}: {param.data.shape} | Number of parameters: {num_params}\")\n",
        "    print(f\"\\nTotal number of parameters in the model: {total_params}\")\n",
        "    print(f\"\\nUSE_PRETRAINED_MODEL={USE_PRETRAINED_MODEL}\\n\")\n",
        "\n",
        "    # Define loss function and optimiser\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimiser = torch.optim.AdamW(model.parameters(), lr=3e-5, weight_decay=1e-4)\n",
        "\n",
        "    # Train and evaluate the model\n",
        "    train_model(model, MODEL_ARCHITECTURE, train_loader, criterion, optimiser, num_epochs=10)\n",
        "    evaluate_model(model, MODEL_ARCHITECTURE, test_loader, device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "m2UO-l9jksvZ",
        "outputId": "dd3896d0-e5e2-4d07-969c-b3ea934e59bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "READING sentence embeddings...\n",
            "LOADING data from /content/drive/MyDrive/ohis/visual7w-text/v7w.TrainImages.itm.txt\n",
            "=========================================\n",
            "|image_data|=9780\n",
            "|question_data|=9780\n",
            "|answer_data|=9780\n",
            "done loading data...\n",
            "LOADING data from /content/drive/MyDrive/ohis/visual7w-text/v7w.TestImages.itm.txt\n",
            "=========================================\n",
            "|image_data|=5980\n",
            "|question_data|=5980\n",
            "|answer_data|=5980\n",
            "done loading data...\n",
            "BUILDING CNN model, pretrained=True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 158MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Architecture:\n",
            "ITM_Model(\n",
            "  (vision_model): ResNet(\n",
            "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): Linear(in_features=512, out_features=128, bias=True)\n",
            "  )\n",
            "  (question_embedding_layer): Linear(in_features=768, out_features=128, bias=True)\n",
            "  (answer_embedding_layer): Linear(in_features=768, out_features=128, bias=True)\n",
            "  (fc): Linear(in_features=384, out_features=2, bias=True)\n",
            ")\n",
            "\n",
            "Model Trainable Parameters:\n",
            "vision_model.fc.weight: torch.Size([128, 512]) | Number of parameters: 65536\n",
            "vision_model.fc.bias: torch.Size([128]) | Number of parameters: 128\n",
            "question_embedding_layer.weight: torch.Size([128, 768]) | Number of parameters: 98304\n",
            "question_embedding_layer.bias: torch.Size([128]) | Number of parameters: 128\n",
            "answer_embedding_layer.weight: torch.Size([128, 768]) | Number of parameters: 98304\n",
            "answer_embedding_layer.bias: torch.Size([128]) | Number of parameters: 128\n",
            "fc.weight: torch.Size([2, 384]) | Number of parameters: 768\n",
            "fc.bias: torch.Size([2]) | Number of parameters: 2\n",
            "\n",
            "Total number of parameters in the model: 263298\n",
            "\n",
            "USE_PRETRAINED_MODEL=True\n",
            "\n",
            "TRAINING CNN model\n",
            "Epoch [1/10], Batch [0/612], Loss: 0.6569\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-1e43842f2093>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# Train and evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMODEL_ARCHITECTURE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimiser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMODEL_ARCHITECTURE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-ffb727202757>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, ARCHITECTURE, train_loader, criterion, optimiser, num_epochs)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;31m# Forward pass -- given input data to the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;31m# Calculate loss (error)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-021677cd84ba>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img, question_embedding, answer_embedding)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mimg_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvision_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mARCHITECTURE\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"ViT\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mimg_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_vit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_features\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Use the custom linear layer for ViT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0midentity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             )\n\u001b[0;32m--> 549\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-O0kA1gSkssa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GzvM2qdVkf7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8YVEsk6akf5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-5tfX5wSkf2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g-HylSDUkfzM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}