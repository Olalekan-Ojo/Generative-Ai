{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Olalekan-Ojo/Generative-Ai/blob/main/ITM%20-%20Image%20Text%20Matching-%20with%20argumentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "fYMEeyo9BOdP"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# Image-Text Matching Classifier: baseline system for visual question answering\n",
        "#\n",
        "# This program has been adapted and rewriten from the CMP9137 materials of 2024.\n",
        "#\n",
        "# It treats the task of multi-choice visual question answering as a binary\n",
        "# classification task. This is possible by rewriting the questions from this format:\n",
        "# v7w_2358727.jpg\tWhen was this?  Nighttime. | Daytime. | Dawn. Sunset.\n",
        "#\n",
        "# to the following format:\n",
        "# v7w_2358727.jpg\tWhen was this? Nighttime. \tmatch\n",
        "# v7w_2358727.jpg\tWhen was this?  Daytime. \tno-match\n",
        "# v7w_2358727.jpg\tWhen was this?  Dawn. \tno-match\n",
        "# v7w_2358727.jpg\tWhen was this?  Sunset.\tno-match\n",
        "#\n",
        "# The list above contains the image file name, the question-answer pairs, and the labels.\n",
        "# Only question types \"when\", \"where\" and \"who\" were used due to compute requirements. In\n",
        "# this folder, files v7w.*Images.itm.txt are used and v7w.*Images.txt are ignored. The\n",
        "# two formats are provided for your information and convenience.\n",
        "#\n",
        "# To enable the above this implementation provides the following classes and functions:\n",
        "# - Class ITM_Dataset() to load the multimodal data (image & text (question and answer)).\n",
        "# - Class Transformer_VisionEncoder() to create a pre-trained Vision Transformer, which\n",
        "#   can be finetuned or trained from scratch -- update USE_PRETRAINED_MODEL accordingly.\n",
        "# - Function load_sentence_embeddings() to load pre-generated sentence embeddings of questions\n",
        "#   and answers, which were generated using SentenceTransformer('sentence-transformers/gtr-t5-large').\n",
        "# - Class ITM_Model() to create a model combining the vision and text encoders above.\n",
        "# - Function train_model trains/finetunes one of two possible models: CNN or ViT. The CNN\n",
        "#   model is based on resnet18, and the Vision Transformer (ViT) is based on vit_b_32.\n",
        "# - Function evaluate_model() calculates the accuracy of the selected model using test data.\n",
        "# - The last block of code brings everything together calling all classes & functions above.\n",
        "#\n",
        "# info of resnet18: https://pytorch.org/vision/main/models/resnet.html\n",
        "# info of vit_b_32: https://pytorch.org/vision/main/models/vision_transformer.html\n",
        "# info of SentenceTransformer: https://huggingface.co/sentence-transformers/gtr-t5-large\n",
        "#\n",
        "# This program was tested on Windows 11 using WSL and does not generate any plots.\n",
        "# Feel free to use and extend this program as part of your our assignment work.\n",
        "#\n",
        "# Version 1.0, main functionality in tensorflow tested with COCO data\n",
        "# Version 1.2, extended functionality for Flickr data\n",
        "# Version 1.3, ported to pytorch and tested with visual7w data\n",
        "# Contact: {hcuayahuitl}@lincoln.ac.uk\n",
        "################################################################################\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import pickle\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.models import vit_b_32\n"
      ],
      "metadata": {
        "id": "CJxxnQTfBkgl"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Simple code to import google drive folder\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5PZbj5cCa9I",
        "outputId": "b17c17a0-1000-40f8-b04d-253f9809b2d4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # prompt: i imported my google drive folder, i need to navidate to the folder ohis to my zipped dataset call ITM_classifier\n",
        "\n",
        "# # Assuming your zipped dataset is in 'My Drive/ohis'\n",
        "# zip_file_path = '/content/drive/MyDrive/ohis/ITM_Classifier-baselines.zip'\n",
        "\n",
        "# # Check if the zip file exists\n",
        "# if os.path.exists(zip_file_path):\n",
        "#   print(f\"Zip file found at: {zip_file_path}\")\n",
        "\n",
        "# else:\n",
        "#   print(f\"Error: Zip file not found at {zip_file_path}. Please check the path.\")\n"
      ],
      "metadata": {
        "id": "0zGW8u0oCXCi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9f8e640-6ab6-4a0a-fc08-bcbef58d1536"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zip file found at: /content/drive/MyDrive/ohis/ITM_Classifier-baselines.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # prompt: code to unzip my folder - zip_file_path\n",
        "\n",
        "# import zipfile\n",
        "# import os\n",
        "\n",
        "# # Assuming your zipped dataset is in 'My Drive/ohis'\n",
        "# zip_file_path = '/content/drive/MyDrive/ohis/ITM_Classifier-baselines.zip'\n",
        "\n",
        "# # Check if the zip file exists\n",
        "# if os.path.exists(zip_file_path):\n",
        "#   print(f\"Zip file found at: {zip_file_path}\")\n",
        "#   try:\n",
        "#     with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "#       zip_ref.extractall('/content/drive/MyDrive/ohis/') # Extract to the same directory\n",
        "#     print(\"Files extracted successfully!\")\n",
        "\n",
        "#   except zipfile.BadZipFile:\n",
        "#     print(f\"Error: The file at {zip_file_path} is not a valid zip file.\")\n",
        "\n",
        "#   except Exception as e:\n",
        "#     print(f\"An error occurred during extraction: {e}\")\n",
        "# else:\n",
        "#   print(f\"Error: Zip file not found at {zip_file_path}. Please check the path.\")\n"
      ],
      "metadata": {
        "id": "Zn1l7v8TFfMg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0814ee7d-8c15-4202-fd70-b8cae1e292a9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zip file found at: /content/drive/MyDrive/ohis/ITM_Classifier-baselines.zip\n",
            "Files extracted successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Dataset\n",
        "class ITM_Dataset(Dataset):\n",
        "    def __init__(self, images_path, data_file, sentence_embeddings, data_split, train_ratio=1.0):\n",
        "        self.images_path = images_path\n",
        "        self.data_file = data_file\n",
        "        self.sentence_embeddings = sentence_embeddings\n",
        "        self.data_split = data_split.lower()\n",
        "        self.train_ratio = train_ratio if self.data_split == \"train\" else 1.0\n",
        "\n",
        "        self.image_data = []\n",
        "        self.question_data = []\n",
        "        self.answer_data = []\n",
        "        self.question_embeddings_data = []\n",
        "        self.answer_embeddings_data = []\n",
        "        self.label_data = []\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((256, 256)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.RandomCrop(224),     # Random crop for training - changed\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            transforms.RandomRotation(10),\n",
        "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Standard for pretrained models on ImageNet\n",
        "        ]) if data_split == \"train\" else transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "        self.load_data()"
      ],
      "metadata": {
        "id": "be220gMBBmpj"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Dataset\n",
        "class ITM_Dataset(Dataset):\n",
        "    def __init__(self, images_path, data_file, sentence_embeddings, data_split, train_ratio=1.0):\n",
        "        self.images_path = images_path\n",
        "        self.data_file = data_file\n",
        "        self.sentence_embeddings = sentence_embeddings\n",
        "        self.data_split = data_split.lower()\n",
        "        self.train_ratio = train_ratio if self.data_split == \"train\" else 1.0\n",
        "\n",
        "        self.image_data = []\n",
        "        self.question_data = []\n",
        "        self.answer_data = []\n",
        "        self.question_embeddings_data = []\n",
        "        self.answer_embeddings_data = []\n",
        "        self.label_data = []\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Standard for pretrained models on ImageNet\n",
        "        ])\n",
        "\n",
        "        self.load_data()\n",
        "\n",
        "    def load_data(self):\n",
        "        print(\"LOADING data from \"+str(self.data_file))\n",
        "        print(\"=========================================\")\n",
        "\n",
        "        random.seed(42)\n",
        "\n",
        "        with open(self.data_file) as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "            # Apply train_ratio only for training data\n",
        "            if self.data_split == \"train\":\n",
        "                random.shuffle(lines)  # Shuffle before selecting\n",
        "                num_samples = int(len(lines) * self.train_ratio)\n",
        "                lines = lines[:num_samples]\n",
        "\n",
        "            for line in lines:\n",
        "                line = line.rstrip(\"\\n\")\n",
        "                img_name, text, raw_label = line.split(\"\\t\")\n",
        "                img_path = os.path.join(self.images_path, img_name.strip())\n",
        "\n",
        "                question_answer_text = text.split(\"?\")\n",
        "                question_text = question_answer_text[0].strip() + '?'\n",
        "                answer_text = question_answer_text[1].strip()\n",
        "\n",
        "                # Get binary labels from match/no-match answers\n",
        "                label = 1 if raw_label == \"match\" else 0\n",
        "                self.image_data.append(img_path)\n",
        "                self.question_data.append(question_text)\n",
        "                self.answer_data.append(answer_text)\n",
        "                self.question_embeddings_data.append(self.sentence_embeddings[question_text])\n",
        "                self.answer_embeddings_data.append(self.sentence_embeddings[answer_text])\n",
        "                self.label_data.append(label)\n",
        "\n",
        "        print(\"|image_data|=\"+str(len(self.image_data)))\n",
        "        print(\"|question_data|=\"+str(len(self.question_data)))\n",
        "        print(\"|answer_data|=\"+str(len(self.answer_data)))\n",
        "        print(\"done loading data...\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_data[idx]\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        img = self.transform(img)\n",
        "        question_embedding = torch.tensor(self.question_embeddings_data[idx], dtype=torch.float32)\n",
        "        answer_embedding = torch.tensor(self.answer_embeddings_data[idx], dtype=torch.float32)\n",
        "        label = torch.tensor(self.label_data[idx], dtype=torch.long)\n",
        "        return img, question_embedding, answer_embedding, label\n"
      ],
      "metadata": {
        "id": "YdkP0lLzkgIF"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load sentence embeddings from an existing file -- generated a priori\n",
        "def load_sentence_embeddings(file_path):\n",
        "    print(\"READING sentence embeddings...\")\n",
        "    with open(file_path, 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "    return data\n",
        "\n",
        "# Pre-trained ViT model\n",
        "class Transformer_VisionEncoder(nn.Module):\n",
        "    def __init__(self, pretrained=None):\n",
        "        super(Transformer_VisionEncoder, self).__init__()\n",
        "\n",
        "        if pretrained:\n",
        "            self.vision_model = vit_b_32(weights=\"IMAGENET1K_V1\")\n",
        "            # Freeze all layers initially\n",
        "            for param in self.vision_model.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "            # Unfreeze the last two layers\n",
        "            for param in list(self.vision_model.heads.parameters())[-2:]:\n",
        "                param.requires_grad = True\n",
        "        else:\n",
        "            self.vision_model = vit_b_32(weights=None)  # Initialize without pretrained weights\n",
        "\n",
        "        # Get feature size after initialising the model\n",
        "        self.num_features = self.vision_model.heads[0].in_features\n",
        "\n",
        "        # Remove original classification head\n",
        "        self.vision_model.heads = nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.vision_model(x)  # Shape should be (batch_size, num_features)\n",
        "        return features\n"
      ],
      "metadata": {
        "id": "U6AXcagpkgEZ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Image-Text Matching Model\n",
        "class ITM_Model(nn.Module):\n",
        "    def __init__(self, num_classes=2, ARCHITECTURE=None, PRETRAINED=None, dropout_rate=0.3): #changed\n",
        "        print(f'BUILDING %s model, pretrained=%s' % (ARCHITECTURE, PRETRAINED))\n",
        "        super(ITM_Model, self).__init__()\n",
        "        self.ARCHITECTURE = ARCHITECTURE\n",
        "\n",
        "        if self.ARCHITECTURE == \"CNN\":\n",
        "            self.vision_model = models.resnet18(pretrained=PRETRAINED)\n",
        "            if PRETRAINED:\n",
        "\t\t\t    # Freeze all layers\n",
        "                for param in self.vision_model.parameters():\n",
        "                    param.requires_grad = False\n",
        "                # Unfreeze the last two layers\n",
        "                for param in list(self.vision_model.children())[-2:]:\n",
        "                    for p in param.parameters():\n",
        "                        p.requires_grad = True\n",
        "            else:\n",
        "                for param in self.vision_model.parameters():\n",
        "                    param.requires_grad = True\n",
        "            self.vision_model.fc = nn.Linear(self.vision_model.fc.in_features, 128) # Change output\n",
        "\n",
        "        elif self.ARCHITECTURE == \"ViT\":\n",
        "            self.vision_model = Transformer_VisionEncoder(pretrained=PRETRAINED)\n",
        "            self.fc_vit = nn.Linear(self.vision_model.num_features, 128)  # Reduce features\n",
        "\n",
        "        else:\n",
        "            print(\"UNKNOWN neural architecture\", ARCHITECTURE)\n",
        "            exit(0)\n",
        "\n",
        "         # Enhanced feature processing\n",
        "        self.question_embedding_layer = nn.Sequential(\n",
        "            nn.Linear(768, 256),\n",
        "            nn.LayerNorm(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(256, 128)\n",
        "        )\n",
        "\n",
        "        self.answer_embedding_layer = nn.Sequential(\n",
        "            nn.Linear(768, 256),\n",
        "            nn.LayerNorm(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(256, 128)\n",
        "        )\n",
        "\n",
        "        # Cross-attention mechanism\n",
        "        self.cross_attention = nn.MultiheadAttention(128, num_heads=4, dropout=dropout_rate)\n",
        "\n",
        "        # Final classification layers\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(128 * 3, 256),\n",
        "            nn.LayerNorm(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, img, question_embedding, answer_embedding):\n",
        "        img_features = self.vision_model(img)\n",
        "        if self.ARCHITECTURE == \"ViT\":\n",
        "            img_features = self.fc_vit(img_features) # Use the custom linear layer for ViT\n",
        "        question_features = self.question_embedding_layer(question_embedding)\n",
        "        answer_features = self.answer_embedding_layer(answer_embedding)\n",
        "\n",
        "        #---------------------------------------- changed\n",
        "        # Apply cross-attention\n",
        "        attn_output, _ = self.cross_attention(\n",
        "            img_features.unsqueeze(0),\n",
        "            torch.stack([question_features, answer_features], dim=0),\n",
        "            torch.stack([question_features, answer_features], dim=0)\n",
        "        )\n",
        "\n",
        "        combined_features = torch.cat((\n",
        "            img_features,\n",
        "            question_features,\n",
        "            answer_features\n",
        "        ), dim=1)\n",
        "\n",
        "        return self.classifier(combined_features)\n",
        ""
      ],
      "metadata": {
        "id": "uXlHvbrQkgBR"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, ARCHITECTURE, train_loader, criterion, optimizer, num_epochs=10):\n",
        "    print(f'TRAINING {ARCHITECTURE} model')\n",
        "\n",
        "    # Add learning rate scheduler\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=3e-4,\n",
        "        epochs=num_epochs,\n",
        "        steps_per_epoch=len(train_loader)\n",
        "    )\n",
        "\n",
        "    # Add gradient scaler for mixed precision training\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "#----------------------------------------- Changed above\n",
        "\n",
        "    # Track the overall loss for each epoch\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        total_batches = len(train_loader)\n",
        "        start_time = time.time()\n",
        "\n",
        "        for batch_idx, (images, question_embeddings, answer_embeddings, labels) in enumerate(train_loader):\n",
        "            # Move images/text/labels to the GPU (if available)\n",
        "            images = images.to(device)\n",
        "            question_embeddings = question_embeddings.to(device)\n",
        "            answer_embeddings = answer_embeddings.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # # Forward pass -- given input data to the model\n",
        "            # outputs = model(images, question_embeddings, answer_embeddings)\n",
        "\n",
        "            # # Calculate loss (error)\n",
        "            # loss = criterion(outputs, labels)  # output should be raw logits\n",
        "\n",
        "            # # Backward pass -- given loss above\n",
        "            # optimiser.zero_grad() # clear the gradients\n",
        "            # loss.backward() # computes gradient of the loss/error\n",
        "            # optimiser.step() # updates parameters using gradients\n",
        "\n",
        "            # Mixed precision training\n",
        "            with torch.cuda.amp.autocast():\n",
        "                outputs = model(images, question_embeddings, answer_embeddings)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            scheduler.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            #-------------------------changed above\n",
        "\n",
        "            if batch_idx % 100 == 0:\n",
        "              print(f'Epoch [{epoch + 1}/{num_epochs}], Batch [{batch_idx}/{total_batches}], '\n",
        "                      f'Loss: {loss.item():.4f}, LR: {scheduler.get_last_lr()[0]:.6f}')\n",
        "\n",
        "\n",
        "\n",
        "        # Print average loss for the epoch\n",
        "        print(f'Epoch [{epoch + 1}/{num_epochs}], Batch [{batch_idx}/{total_batches}], '\n",
        "                      f'Loss: {loss.item():.4f}, LR: {scheduler.get_last_lr()[0]:.6f}')\n",
        "\n",
        "        avg_loss = running_loss / total_batches\n",
        "        print(f'Epoch [{epoch + 1}/{num_epochs}] Average Loss: {avg_loss:.4f}')\n"
      ],
      "metadata": {
        "id": "3eN2aSkGkf_F"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, ARCHITECTURE, test_loader, device):\n",
        "    print(f'EVALUATING %s model' % (ARCHITECTURE))\n",
        "    model.eval()\n",
        "    total_test_loss = 0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    start_time = time.time()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, question_embeddings, answer_embeddings, labels in test_loader:\n",
        "            # Move images/text/labels to the GPU (if available)\n",
        "            images = images.to(device)\n",
        "            question_embeddings = question_embeddings.to(device)\n",
        "            answer_embeddings = answer_embeddings.to(device)\n",
        "            labels = labels.to(device)  # Labels are single integers (0 or 1)\n",
        "\n",
        "            # Perform forward pass on our data\n",
        "            outputs = model(images, question_embeddings, answer_embeddings)\n",
        "\n",
        "            # Accumulate loss on test data\n",
        "            total_test_loss += criterion(outputs, labels)\n",
        "\n",
        "            # Since outputs are logits, apply softmax to get probabilities\n",
        "            predicted_probabilities = torch.softmax(outputs, dim=1)  # Use softmax for multi-class output\n",
        "            predicted_class = predicted_probabilities.argmax(dim=1)  # Get the predicted class index (0 or 1)\n",
        "\n",
        "            # Store labels and predictions for later analysis\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted_class.cpu().numpy())\n",
        "\n",
        "    # Convert to numpy arrays for easier calculations\n",
        "    all_labels = np.array(all_labels)\n",
        "    all_predictions = np.array(all_predictions)\n",
        "\n",
        "    # Calculate true positives, true negatives, false positives, false negatives\n",
        "    tp = np.sum((all_predictions == 1) & (all_labels == 1))  # True positives\n",
        "    tn = np.sum((all_predictions == 0) & (all_labels == 0))  # True negatives\n",
        "    fp = np.sum((all_predictions == 1) & (all_labels == 0))  # False positives\n",
        "    fn = np.sum((all_predictions == 0) & (all_labels == 1))  # False negatives\n",
        "\n",
        "    # Calculate sensitivity, specificity, and balanced accuracy\n",
        "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
        "    balanced_accuracy = (sensitivity + specificity) / 2.0\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f'Balanced Accuracy: {balanced_accuracy:.4f}, {elapsed_time:.2f} seconds')\n",
        "    print(f'Total Test Loss: {total_test_loss:.4f}')\n",
        "\n"
      ],
      "metadata": {
        "id": "UKUcjEY_ksyA"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main Execution\n",
        "if __name__ == '__main__':\n",
        "    # Check GPU availability\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f'Using device: {device}')\n",
        "\n",
        "    # Paths and files\n",
        "    # Paths and files\n",
        "    IMAGES_PATH = \"/content/drive/MyDrive/ohis/visual7w-images\"\n",
        "    train_data_file = \"/content/drive/MyDrive/ohis/visual7w-text/v7w.TrainImages.itm.txt\"\n",
        "    dev_data_file = \"/content/drive/MyDrive/ohis/visual7w-text/v7w.DevImages.itm.txt\"\n",
        "    test_data_file = \"/content/drive/MyDrive/ohis/visual7w-text/v7w.TestImages.itm.txt\"\n",
        "    sentence_embeddings_file = \"/content/drive/MyDrive/ohis/v7w.sentence_embeddings-gtr-t5-large.pkl\"\n",
        "    sentence_embeddings = load_sentence_embeddings(sentence_embeddings_file)\n",
        "\n",
        "    # Create datasets and loaders\n",
        "    train_dataset = ITM_Dataset(IMAGES_PATH, train_data_file, sentence_embeddings, data_split=\"train\", train_ratio=0.2)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
        "    test_dataset = ITM_Dataset(IMAGES_PATH, test_data_file, sentence_embeddings, data_split=\"test\")  # whole test data\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
        "\n",
        "    # The dev set is not used in this program and you should/could use it for example to optimise your hyperparameters\n",
        "    #dev_dataset = ITM_Dataset(images_path, \"dev_data.txt\", sentence_embeddings, data_split=\"dev\")  # whole dev data\n",
        "\n",
        "    # Create the model using one of the two supported architectures\n",
        "    MODEL_ARCHITECTURE = \"ViT\"  # ViT often performs better for this task\n",
        "    USE_PRETRAINED_MODEL = True\n",
        "\n",
        "    # Create model with dropout\n",
        "    model = ITM_Model(\n",
        "        num_classes=2,\n",
        "        ARCHITECTURE=MODEL_ARCHITECTURE,\n",
        "        PRETRAINED=USE_PRETRAINED_MODEL,\n",
        "        dropout_rate=0.3\n",
        "    ).to(device)\n",
        "\n",
        "    # Use a weighted loss function to handle class imbalance\n",
        "    class_weights = torch.tensor([1.0, 2.0]).to(device)  # Adjust based on your class distribution\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "    # Use AdamW with weight decay\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=3e-5,\n",
        "        weight_decay=0.01,\n",
        "        betas=(0.9, 0.999)\n",
        "    )\n",
        "\n",
        "\n",
        "    # Increase epochs and use larger batch size\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
        "\n",
        "    train_model(model, MODEL_ARCHITECTURE, train_loader, criterion, optimizer, num_epochs=20)\n",
        "    evaluate_model(model, MODEL_ARCHITECTURE, test_loader, device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2UO-l9jksvZ",
        "outputId": "220ac482-25d4-4289-e839-d6bd3f62aa4c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "READING sentence embeddings...\n",
            "LOADING data from /content/drive/MyDrive/ohis/visual7w-text/v7w.TrainImages.itm.txt\n",
            "=========================================\n",
            "|image_data|=9780\n",
            "|question_data|=9780\n",
            "|answer_data|=9780\n",
            "done loading data...\n",
            "LOADING data from /content/drive/MyDrive/ohis/visual7w-text/v7w.TestImages.itm.txt\n",
            "=========================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|image_data|=5980\n",
            "|question_data|=5980\n",
            "|answer_data|=5980\n",
            "done loading data...\n",
            "BUILDING ViT model, pretrained=True\n",
            "TRAINING ViT model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-e810a17d6e9f>:13: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n",
            "<ipython-input-26-e810a17d6e9f>:42: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Batch [0/306], Loss: 0.8167, LR: 0.000012\n",
            "Epoch [1/20], Batch [100/306], Loss: 0.7500, LR: 0.000014\n",
            "Epoch [1/20], Batch [200/306], Loss: 0.6718, LR: 0.000020\n",
            "Epoch [1/20], Batch [300/306], Loss: 0.5642, LR: 0.000031\n",
            "Epoch [1/20], Batch [305/306], Loss: 0.8502, LR: 0.000031\n",
            "Epoch [1/20] Average Loss: 0.6839\n",
            "Epoch [2/20], Batch [0/306], Loss: 0.7288, LR: 0.000031\n",
            "Epoch [2/20], Batch [100/306], Loss: 0.7569, LR: 0.000046\n",
            "Epoch [2/20], Batch [200/306], Loss: 0.6028, LR: 0.000063\n",
            "Epoch [2/20], Batch [300/306], Loss: 0.4362, LR: 0.000083\n",
            "Epoch [2/20], Batch [305/306], Loss: 0.7542, LR: 0.000084\n",
            "Epoch [2/20] Average Loss: 0.6100\n",
            "Epoch [3/20], Batch [0/306], Loss: 0.4442, LR: 0.000084\n",
            "Epoch [3/20], Batch [100/306], Loss: 0.6364, LR: 0.000107\n",
            "Epoch [3/20], Batch [200/306], Loss: 0.6359, LR: 0.000130\n",
            "Epoch [3/20], Batch [300/306], Loss: 0.5446, LR: 0.000155\n",
            "Epoch [3/20], Batch [305/306], Loss: 0.6230, LR: 0.000156\n",
            "Epoch [3/20] Average Loss: 0.5580\n",
            "Epoch [4/20], Batch [0/306], Loss: 0.4286, LR: 0.000156\n",
            "Epoch [4/20], Batch [100/306], Loss: 0.7107, LR: 0.000181\n",
            "Epoch [4/20], Batch [200/306], Loss: 0.6986, LR: 0.000205\n",
            "Epoch [4/20], Batch [300/306], Loss: 0.4388, LR: 0.000227\n",
            "Epoch [4/20], Batch [305/306], Loss: 0.4332, LR: 0.000228\n",
            "Epoch [4/20] Average Loss: 0.5278\n",
            "Epoch [5/20], Batch [0/306], Loss: 0.5495, LR: 0.000228\n",
            "Epoch [5/20], Batch [100/306], Loss: 0.6258, LR: 0.000249\n",
            "Epoch [5/20], Batch [200/306], Loss: 0.4658, LR: 0.000266\n",
            "Epoch [5/20], Batch [300/306], Loss: 0.5036, LR: 0.000280\n",
            "Epoch [5/20], Batch [305/306], Loss: 0.7153, LR: 0.000281\n",
            "Epoch [5/20] Average Loss: 0.5142\n",
            "Epoch [6/20], Batch [0/306], Loss: 0.4522, LR: 0.000281\n",
            "Epoch [6/20], Batch [100/306], Loss: 0.3771, LR: 0.000291\n",
            "Epoch [6/20], Batch [200/306], Loss: 0.4843, LR: 0.000298\n",
            "Epoch [6/20], Batch [300/306], Loss: 0.4624, LR: 0.000300\n",
            "Epoch [6/20], Batch [305/306], Loss: 0.3692, LR: 0.000300\n",
            "Epoch [6/20] Average Loss: 0.4913\n",
            "Epoch [7/20], Batch [0/306], Loss: 0.3488, LR: 0.000300\n",
            "Epoch [7/20], Batch [100/306], Loss: 0.5579, LR: 0.000300\n",
            "Epoch [7/20], Batch [200/306], Loss: 0.2224, LR: 0.000298\n",
            "Epoch [7/20], Batch [300/306], Loss: 0.6151, LR: 0.000296\n",
            "Epoch [7/20], Batch [305/306], Loss: 0.3101, LR: 0.000296\n",
            "Epoch [7/20] Average Loss: 0.4652\n",
            "Epoch [8/20], Batch [0/306], Loss: 0.3478, LR: 0.000296\n",
            "Epoch [8/20], Batch [100/306], Loss: 0.2732, LR: 0.000293\n",
            "Epoch [8/20], Batch [200/306], Loss: 0.2889, LR: 0.000290\n",
            "Epoch [8/20], Batch [300/306], Loss: 0.3463, LR: 0.000285\n",
            "Epoch [8/20], Batch [305/306], Loss: 0.4243, LR: 0.000285\n",
            "Epoch [8/20] Average Loss: 0.4387\n",
            "Epoch [9/20], Batch [0/306], Loss: 0.3795, LR: 0.000285\n",
            "Epoch [9/20], Batch [100/306], Loss: 0.2924, LR: 0.000280\n",
            "Epoch [9/20], Batch [200/306], Loss: 0.3230, LR: 0.000274\n",
            "Epoch [9/20], Batch [300/306], Loss: 0.5404, LR: 0.000268\n",
            "Epoch [9/20], Batch [305/306], Loss: 0.1948, LR: 0.000267\n",
            "Epoch [9/20] Average Loss: 0.4034\n",
            "Epoch [10/20], Batch [0/306], Loss: 0.2729, LR: 0.000267\n",
            "Epoch [10/20], Batch [100/306], Loss: 0.4765, LR: 0.000260\n",
            "Epoch [10/20], Batch [200/306], Loss: 0.3822, LR: 0.000252\n",
            "Epoch [10/20], Batch [300/306], Loss: 0.3228, LR: 0.000244\n",
            "Epoch [10/20], Batch [305/306], Loss: 0.4947, LR: 0.000243\n",
            "Epoch [10/20] Average Loss: 0.3765\n",
            "Epoch [11/20], Batch [0/306], Loss: 0.2625, LR: 0.000243\n",
            "Epoch [11/20], Batch [100/306], Loss: 0.4886, LR: 0.000234\n",
            "Epoch [11/20], Batch [200/306], Loss: 0.2252, LR: 0.000225\n",
            "Epoch [11/20], Batch [300/306], Loss: 0.3127, LR: 0.000215\n",
            "Epoch [11/20], Batch [305/306], Loss: 0.3302, LR: 0.000215\n",
            "Epoch [11/20] Average Loss: 0.3341\n",
            "Epoch [12/20], Batch [0/306], Loss: 0.2251, LR: 0.000215\n",
            "Epoch [12/20], Batch [100/306], Loss: 0.2821, LR: 0.000205\n",
            "Epoch [12/20], Batch [200/306], Loss: 0.1892, LR: 0.000194\n",
            "Epoch [12/20], Batch [300/306], Loss: 0.3263, LR: 0.000184\n",
            "Epoch [12/20], Batch [305/306], Loss: 0.3443, LR: 0.000183\n",
            "Epoch [12/20] Average Loss: 0.3014\n",
            "Epoch [13/20], Batch [0/306], Loss: 0.2842, LR: 0.000183\n",
            "Epoch [13/20], Batch [100/306], Loss: 0.2155, LR: 0.000172\n",
            "Epoch [13/20], Batch [200/306], Loss: 0.3565, LR: 0.000161\n",
            "Epoch [13/20], Batch [300/306], Loss: 0.1870, LR: 0.000150\n",
            "Epoch [13/20], Batch [305/306], Loss: 0.4754, LR: 0.000150\n",
            "Epoch [13/20] Average Loss: 0.2807\n",
            "Epoch [14/20], Batch [0/306], Loss: 0.1763, LR: 0.000150\n",
            "Epoch [14/20], Batch [100/306], Loss: 0.2396, LR: 0.000139\n",
            "Epoch [14/20], Batch [200/306], Loss: 0.3953, LR: 0.000128\n",
            "Epoch [14/20], Batch [300/306], Loss: 0.1816, LR: 0.000117\n",
            "Epoch [14/20], Batch [305/306], Loss: 0.3451, LR: 0.000117\n",
            "Epoch [14/20] Average Loss: 0.2646\n",
            "Epoch [15/20], Batch [0/306], Loss: 0.1625, LR: 0.000116\n",
            "Epoch [15/20], Batch [100/306], Loss: 0.1337, LR: 0.000106\n",
            "Epoch [15/20], Batch [200/306], Loss: 0.3791, LR: 0.000095\n",
            "Epoch [15/20], Batch [300/306], Loss: 0.1555, LR: 0.000085\n",
            "Epoch [15/20], Batch [305/306], Loss: 0.1052, LR: 0.000085\n",
            "Epoch [15/20] Average Loss: 0.2243\n",
            "Epoch [16/20], Batch [0/306], Loss: 0.3152, LR: 0.000085\n",
            "Epoch [16/20], Batch [100/306], Loss: 0.1166, LR: 0.000075\n",
            "Epoch [16/20], Batch [200/306], Loss: 0.2359, LR: 0.000066\n",
            "Epoch [16/20], Batch [300/306], Loss: 0.1063, LR: 0.000057\n",
            "Epoch [16/20], Batch [305/306], Loss: 0.2144, LR: 0.000056\n",
            "Epoch [16/20] Average Loss: 0.1924\n",
            "Epoch [17/20], Batch [0/306], Loss: 0.2312, LR: 0.000056\n",
            "Epoch [17/20], Batch [100/306], Loss: 0.1554, LR: 0.000048\n",
            "Epoch [17/20], Batch [200/306], Loss: 0.2228, LR: 0.000040\n",
            "Epoch [17/20], Batch [300/306], Loss: 0.0855, LR: 0.000033\n",
            "Epoch [17/20], Batch [305/306], Loss: 0.0979, LR: 0.000033\n",
            "Epoch [17/20] Average Loss: 0.1648\n",
            "Epoch [18/20], Batch [0/306], Loss: 0.1602, LR: 0.000033\n",
            "Epoch [18/20], Batch [100/306], Loss: 0.0927, LR: 0.000026\n",
            "Epoch [18/20], Batch [200/306], Loss: 0.1719, LR: 0.000020\n",
            "Epoch [18/20], Batch [300/306], Loss: 0.1069, LR: 0.000015\n",
            "Epoch [18/20], Batch [305/306], Loss: 0.3979, LR: 0.000015\n",
            "Epoch [18/20] Average Loss: 0.1511\n",
            "Epoch [19/20], Batch [0/306], Loss: 0.2759, LR: 0.000015\n",
            "Epoch [19/20], Batch [100/306], Loss: 0.2268, LR: 0.000010\n",
            "Epoch [19/20], Batch [200/306], Loss: 0.2198, LR: 0.000007\n",
            "Epoch [19/20], Batch [300/306], Loss: 0.2268, LR: 0.000004\n",
            "Epoch [19/20], Batch [305/306], Loss: 0.2001, LR: 0.000004\n",
            "Epoch [19/20] Average Loss: 0.1432\n",
            "Epoch [20/20], Batch [0/306], Loss: 0.2954, LR: 0.000004\n",
            "Epoch [20/20], Batch [100/306], Loss: 0.1538, LR: 0.000002\n",
            "Epoch [20/20], Batch [200/306], Loss: 0.1351, LR: 0.000000\n",
            "Epoch [20/20], Batch [300/306], Loss: 0.2607, LR: 0.000000\n",
            "Epoch [20/20], Batch [305/306], Loss: 0.1247, LR: 0.000000\n",
            "Epoch [20/20] Average Loss: 0.1387\n",
            "EVALUATING ViT model\n",
            "Balanced Accuracy: 0.7220, 24.81 seconds\n",
            "Total Test Loss: 142.6530\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tkinter as tk\n",
        "from tkinter import filedialog\n",
        "from PIL import Image, ImageTk"
      ],
      "metadata": {
        "id": "Tn4eNM0J-5uH"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_time_of_day(model, image_path, sentence_embeddings, device):\n",
        "    \"\"\"\n",
        "    Predicts the time of day for a given image\n",
        "    \"\"\"\n",
        "    # Image preprocessing\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # Load and preprocess image\n",
        "    try:\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        image = transform(image).unsqueeze(0).to(device)  # Add batch dimension\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading image: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Fixed question and possible answers\n",
        "    question = \"When was this?\"\n",
        "    possible_answers = [\"Nighttime\", \"Daytime\", \"Dawn\", \"Sunset\"]\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Get question embedding (same for all predictions)\n",
        "    question_embedding = torch.tensor(sentence_embeddings[question], dtype=torch.float32)\n",
        "    question_embedding = question_embedding.unsqueeze(0).to(device)\n",
        "\n",
        "    # Test each possible answer\n",
        "    results = []\n",
        "    for answer in possible_answers:\n",
        "        # Get answer embedding\n",
        "        answer_embedding = torch.tensor(sentence_embeddings[answer], dtype=torch.float32)\n",
        "        answer_embedding = answer_embedding.unsqueeze(0).to(device)\n",
        "\n",
        "        # Make prediction\n",
        "        with torch.no_grad():\n",
        "            outputs = model(image, question_embedding, answer_embedding)\n",
        "            probabilities = torch.softmax(outputs, dim=1)\n",
        "            prediction = torch.argmax(probabilities, dim=1)\n",
        "            confidence = probabilities[0][prediction[0]].item()\n",
        "\n",
        "        results.append((answer, confidence, prediction.item()))\n",
        "\n",
        "    # Find the best matching answer\n",
        "    best_match = max(results, key=lambda x: x[1] if x[2] == 1 else 0)\n",
        "    return best_match[0], best_match[1]  # Return time of day and confidence\n",
        "\n",
        "def interactive_image_testing(model, sentence_embeddings, device):\n",
        "    \"\"\"\n",
        "    Interactive loop for testing new images\n",
        "    \"\"\"\n",
        "    print(\"\\n=== Time of Day Image Predictor ===\")\n",
        "    print(\"Enter 'quit' to exit\")\n",
        "\n",
        "    while True:\n",
        "        # Get image path\n",
        "        image_path = input(\"\\nEnter the path to your image: \").strip()\n",
        "        if image_path.lower() == 'quit':\n",
        "            break\n",
        "\n",
        "        # Make prediction\n",
        "        try:\n",
        "            time_of_day, confidence = predict_time_of_day(model, image_path, sentence_embeddings, device)\n",
        "\n",
        "            # Display results\n",
        "            print(\"\\nResults:\")\n",
        "            print(\"-\" * 50)\n",
        "            print(f\"Image: {image_path}\")\n",
        "            print(f\"Predicted Time of Day: {time_of_day}\")\n",
        "            print(f\"Confidence: {confidence:.2%}\")\n",
        "            print(\"-\" * 50)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing image: {e}\")\n",
        "\n",
        "# Add this at the end of your main block:\n",
        "if __name__ == '__main__':\n",
        "    # ... (your existing training code) ...\n",
        "\n",
        "    # After training and evaluation, add:\n",
        "    print(\"\\nWould you like to test new images? (yes/no)\")\n",
        "    response = input().strip().lower()\n",
        "    if response == 'yes':\n",
        "        interactive_image_testing(model, sentence_embeddings, device)"
      ],
      "metadata": {
        "id": "-O0kA1gSkssa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7483d65d-168d-4a96-ae14-1d0b4ca7f7af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Would you like to test new images? (yes/no)\n",
            "yes \n",
            "\n",
            "=== Time of Day Image Predictor ===\n",
            "Enter 'quit' to exit\n",
            "\n",
            "Enter the path to your image: /content/img__01327_.png\n",
            "Error processing image: 'Nighttime'\n",
            "\n",
            "Enter the path to your image: /content/img__01327_.png\n",
            "Error processing image: 'Nighttime'\n",
            "\n",
            "Enter the path to your image: /content/Firefly 20250127010512.png\n",
            "Error processing image: 'Nighttime'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GzvM2qdVkf7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8YVEsk6akf5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-5tfX5wSkf2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g-HylSDUkfzM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}